Text Analytics
1. Extract Sample document and apply following document preprocessing methods:
Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.
2. Create representation of document by calculating Term Frequency and Inverse Document
Frequency.



import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer



# Download necessary NLTK resources
import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')



nltk.download('averaged_perceptron_tagger_eng')sample_doc = """
Natural Language Processing (NLP) is a field of Artificial Intelligence that enables computers to understand, interpret, and generate human language. 
It includes tasks like tokenization, part-of-speech tagging, named entity recognition, and more.
"""




# Tokenization
tokens = word_tokenize(sample_doc)
print("Tokens:", tokens)




# POS Tagging
pos_tags = pos_tag(tokens)
print("POS Tags:", pos_tags)



# Stopwords removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("Tokens after stopwords removal:", filtered_tokens)



# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
print("Tokens after stemming:", stemmed_tokens)



# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("Tokens after lemmatization:", lemmatized_tokens)



# TF calculation
vectorizer = TfidfVectorizer(use_idf=False, norm=None)
X = vectorizer.fit_transform([sample_doc])
tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print("Term Frequency (TF):\n", tf)




# TF calculation
vectorizer = TfidfVectorizer(use_idf=False, norm=None)
X = vectorizer.fit_transform([sample_doc])
tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print("Term Frequency (TF):\n", tf)



